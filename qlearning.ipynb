{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Lab Notebook\n",
    "\n",
    "In this notebook, you will learn the basics applying the Q-Learning algorithm to simple environments in OpenAI Gym.  By the end of the notebook, you will have a working Q-Learning agent that can balance a CartPole as well as solve other simple control problems in alternate environments.\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. <a href=#rl>Basic concept of reinforcement learning (RL)</a>\n",
    "- <a href=#cp>CartPole environment from OpenAI Gym</a>\n",
    "- <a href=#random>CartPole with a random agent</a>\n",
    "- <a href=#qlcode>Q-Learning Implementation in Python</a>\n",
    "- <a href=#qlagent>CartPole with a Q-Learning agent</a>\n",
    "- <a href=#mtn>MountainCar environment from OpenAI Gym</a>\n",
    "\n",
    "## Requirements\n",
    "* OpenAI Gym[classic_control] 0.7.4 or higher \n",
    "* Python 3.5 or higher\n",
    "* CUDA8.0 enabled GPU\n",
    "\n",
    "Portions of this notebook have been borrowed from the Nvidia Deep Learning Institute (DLI) hands-on labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reinforcement learning <a name='rl' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setting\n",
    "\n",
    "Unlike standard supervised learning in machine learning, reinforcement learning is about sequential decision making of an agent, which takes actions to maximize cumulative reward, by interacting with environments (problems).\n",
    "\n",
    "<img src=\"image/rl.png\" width=\"200\">\n",
    "\n",
    "After observing the current state $s_t$ and reward $r_t$, agent will decide which action to take $a_t$.\n",
    "\n",
    "For clarity, here we assume that actions are discrete, and reward is a scalar value. Multi-dimensional state $s_t$ can be either discrete or continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value function approximation\n",
    "\n",
    "There are multiple strategies in RL on how to represent the policy, the behavior mechanism inside the agent and how to optimize it.\n",
    "\n",
    "In most of this hands-on session, we just consider action-value function approximation, in which the expected cumulative reward for all future steps is represented as a function $Q(s, a)$ for pairs of state $s$ and following action $a$.\n",
    "\n",
    "By learning a good approximation of optimal action-value function $Q(s, a)$, by taking action $a_t$ which maximizes the learned Q-value $Q(s_t, a_t)$ given $s_t$ at each time step $t$, the optimal policy should be realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example - CartPole <a name='cp' />\n",
    "\n",
    "In this section, we introduce a control problem named CartPole and describe the way it is handled as a RL problem.  Before looking closer at the CartPole environment, we'll import the packages we need and define a visualization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in /home/shane/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: EasyProcess in /home/shane/anaconda3/lib/python3.6/site-packages (from pyvirtualdisplay)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1009'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1009'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install packages in Udacity Workspaces\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization helpers\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set logger level\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Environment\n",
    "\n",
    "As an example of classical RL problem, we use CartPole-v0 from [OpenAI Gym](https://gym.openai.com/), which contains two-dimensional physics simulator of a black cart and a yellow pole. It is a kind of inverted pendulum.\n",
    "\n",
    "<img src=\"image/cartpole.png?\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following overview of CartPole-v0 is taken from [Gym's Wiki page](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "\n",
    "#### Description\n",
    "By an un-actuated joint, a pole is attached to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "#### Observation\n",
    "Type: Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "#### Actions\n",
    "Type: Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "#### Conditions for episode termination \n",
    "1. Pole Angle is more than ±20.9°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "#### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "#### Starting state\n",
    "All observations are assigned a uniform random value between ±0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Gym's environment works\n",
    "\n",
    "Gym's environment (problem) has a unified interface with the following three steps.\n",
    "\n",
    "#### 1. Create environment specified by name\n",
    "```python\n",
    "env = gym.make('ENV_NAME')\n",
    "```\n",
    "#### 2. Initialize environment\n",
    "\n",
    "```python\n",
    "env.reset()\n",
    "```\n",
    "#### 3. Take action and observe reward & next state\n",
    "```python\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "```\n",
    "where\n",
    "  - obs: a next observation\n",
    "  - reaward: a scalar reward\n",
    "  - is_finished: a boolean value indicating whether the current state is terminal or not\n",
    "  - info: additional information\n",
    "\n",
    "By interacting the environment, a reinforcement learning agent learns how to optimize its strategy for maximizing cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: How the environment works\n",
    "\n",
    "Create the CartPole environment and observe the initial state and the result of an action.  In this case a random action has been selected by sampling the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n",
      "initial observation: [-0.03193025  0.04635147 -0.02156614  0.0198589 ]\n",
      "random action: 0\n",
      "next observation: [-0.03100322 -0.14845467 -0.02116896  0.3056602 ]\n",
      "reward: 1.0\n",
      "is_finished: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "# Create  environment of CartPole-v0\n",
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "observation = env.reset()\n",
    "#env.render(mode='rgb_array', close=True)\n",
    "print('initial observation:', observation)\n",
    "\n",
    "action = env.action_space.sample() # Select random action\n",
    "print('random action:', action)\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "print('next observation:', observation)\n",
    "print('reward:', reward)\n",
    "print('is_finished:', is_finished)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CartPole with the Random agent <a name='random' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copied from gym examples - a model for our agent\n",
    "class RandAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done=None, mode=None):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def init_episode(self, observation):\n",
    "        # provided for compatibility with general learner\n",
    "        return self.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Learner for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This learner provides a general interface for agents and environments \n",
    "# optionally visualize within a Jupyter Notebook when visualize_plt is True\n",
    "def learner(agent=None, env_id='CartPole-v0', episodes=100, max_length = 100, init_reward=0, \n",
    "            ignore_done=False, visualize_plt=True, mode=None):\n",
    "    # load the environment \n",
    "    env = gym.make(env_id)\n",
    "    # set the agent to random if none provided\n",
    "    if agent is None:\n",
    "        agent = RandAgent(env.action_space)\n",
    "\n",
    "    # each episode runs until it is observed as finished, or exceeds max_length in time steps\n",
    "    episode_count = episodes\n",
    "    done = False\n",
    "    n_steps = np.zeros((episode_count,))\n",
    "\n",
    "    # run the episodes - use tqdm to track in the notebook\n",
    "    for i in tqdm(range(episode_count), disable=visualize_plt):\n",
    "        \n",
    "        # Initialize environment for each episode\n",
    "        ob = env.reset()  \n",
    "        reward = init_reward\n",
    "        if visualize_plt:\n",
    "            img = plt.imshow(env.render(mode='rgb_array')) # only call this once, only for jupyter\n",
    "        \n",
    "        # initialize the agent\n",
    "        agent.init_episode(ob)\n",
    "        n_steps[i]=max_length\n",
    "        \n",
    "        # run the steps in each epsisode\n",
    "        for t in range(max_length):\n",
    "            # render the environment\n",
    "            if visualize_plt:\n",
    "                img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "                plt.axis('off')\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                env.render()\n",
    "            \n",
    "            # get agent's action\n",
    "            action = agent.act(ob, reward, mode=mode)\n",
    "            # take the action and get reward and updated observation\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # terminate the steps if the problem is done\n",
    "            if done and not ignore_done:\n",
    "                n_steps[i] = t\n",
    "                break\n",
    "            if done and ignore_done and env_id=='MountainCar-v0' and ob[0]>= 0.5:\n",
    "                # special case MountainCar\n",
    "                # if have achieved the goal, then quit but otherwise keep going\n",
    "                print(\"Episode {} done at step {}\".format(i,t))\n",
    "                print(\"Observations {}, Reward {}\".format(ob, reward))\n",
    "                n_steps[i] = t\n",
    "                break\n",
    "    env.close()\n",
    "    return n_steps  # stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Run the experiment with the random agent\n",
    "\n",
    "Before introducing an RL agent, you can just repeat random actions to see how the environment changes over time. Here you repeat the episode for 10 times.  The number of steps before the cart fails are recorded for each episode.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "max_length = 200\n",
    "steps = learner(episodes=num_episodes, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Traditional RL = Q-learning <a name='qlcode' />\n",
    "\n",
    "Q-learning is one of the most popular approaches in reinforcement learning, to determine which action is optimal on each state.\n",
    "\n",
    "At each time step $t$, after taking an action $a_t$ and given reward $r_{t+1}$ and next state $s_{t+1}$, the old Q-value $Q(s_t, a_t)$ is updated by the following rule. \n",
    "\n",
    "<img src=\"image/q-learning.svg\" width=\"800\">\n",
    "\n",
    "It is not necessary to follow the details of math but the intuition is: the Q-value should be updated by taking into account the difference between the old value and the sum of the actual reward and the discounted estimated future value obtained by next action, by the factor of learning rate $\\alpha_t$.\n",
    "\n",
    "Q-learning is an iterative algorithm, so Q-values are used to take actions and then gradually improved over time.\n",
    "\n",
    "To make the good balance between exploration (random action) and exploitation (estimated best action), $\\epsilon$-greedy strategy is typically used with Q-learning in which where the agent take random actions with probability $\\epsilon$  and current best action otherwise.\n",
    "\n",
    "The simplest way to implement Q-Learning is to build a table, named Q-table, to store the Q-value for each discrete state and action combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Q-learning agent\n",
    "\n",
    "Here we define the QLearningAgent class, in which the observation space is discretized into q-tables of size 20,000 =  ((that of bins (9)+1) ^ numbers of observations (4)) * number of actions (2). Each cell represents current Q-value for each combination of discretized state and possible action.\n",
    "\n",
    "By calling the `act()` method with the current observation (and reward), the agent updates the Q-table and returns the next action.\n",
    "\n",
    "The hyper-parameters for training have been set at default values, but may be modified when called:\n",
    "\n",
    "**`learning_rate` = 0.2**\n",
    "* the new information replaces 20% of the old information at each step\n",
    "\n",
    "**`discount_factor` = 1.0** \n",
    "* the agent will strive for a long-term high reward because the value includes expected future rewards\n",
    "\n",
    "**`exploration_rate` = 0.5**\n",
    "* the agent will choose a random action, i.e. \"explore\", 50% of the time\n",
    "\n",
    "**`exploration_decay_rate` = 0.99**\n",
    "* the probability of exploration will be reduced by 1% at the start of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, \n",
    "                 learning_rate = 0.2, discount_factor = 1.0,\n",
    "                 exploration_rate = 0.5, exploration_decay_rate = 0.99,\n",
    "                 n_bins = 9, n_actions = 2, splits=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate # initial epsilon\n",
    "        self.exploration_decay_rate = exploration_decay_rate # decay factor for epsilon\n",
    "        self.n_bins = n_bins\n",
    "        self.n_actions = n_actions\n",
    "        self.splits = splits\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        \n",
    "        if self.splits is None: #CartPole default\n",
    "            self.splits = [\n",
    "                # Position\n",
    "                np.linspace(-2.4, 2.4, self.n_bins)[1:-1],\n",
    "                # Velocity\n",
    "                np.linspace(-3.5, 3.5, self.n_bins)[1:-1],\n",
    "                # Angle.\n",
    "                np.linspace(-0.5, 0.5, self.n_bins)[1:-1],\n",
    "                # Tip velocity\n",
    "                np.linspace(-2.0, 2.0, self.n_bins)[1:-1]\n",
    "            ]\n",
    "        \n",
    "        # Create Q-Table\n",
    "        num_states = (self.n_bins+1) ** len(self.splits)\n",
    "        self.q_table = np.zeros(shape=(num_states, self.n_actions))\n",
    "\n",
    "    # Turn the observation into integer state\n",
    "    def set_state(self, observation):\n",
    "        state = 0\n",
    "        for i, column in enumerate(observation):\n",
    "            state +=  np.digitize(x=column, bins=self.splits[i]) * ((self.n_bins + 1) ** i)\n",
    "        return state\n",
    "\n",
    "    # Initialize for each episode\n",
    "    def init_episode(self, observation):\n",
    "        # Gradually decrease exploration rate\n",
    "        self.exploration_rate *= self.exploration_decay_rate\n",
    "\n",
    "        # Decide initial action\n",
    "        self.state = self.set_state(observation)\n",
    "        return np.argmax(self.q_table[self.state])\n",
    "\n",
    "    # Select action and update\n",
    "    def act(self, observation, reward=None, done=None, mode='train'):\n",
    "        next_state = self.set_state(observation)\n",
    "        \n",
    "        if mode == 'test':\n",
    "            # Test mode \n",
    "            next_action = np.argmax(self.q_table[next_state])\n",
    "        else:\n",
    "            # Train mode by default\n",
    "            # Train by updating Q-Table based on current reward and 'last' action.\n",
    "            self.q_table[self.state, self.action] += self.learning_rate * \\\n",
    "                (reward + self.discount_factor * max(self.q_table[next_state, :]) - self.q_table[self.state, self.action])\n",
    "            # Exploration or exploitation\n",
    "            do_exploration = (1 - self.exploration_rate) < np.random.uniform(0, 1)\n",
    "            if do_exploration:\n",
    "                #  Exploration\n",
    "                next_action = np.random.randint(0, self.n_actions)\n",
    "            else:\n",
    "                # Exploitation\n",
    "                next_action = np.argmax(self.q_table[next_state])\n",
    "\n",
    "        self.state = next_state\n",
    "        self.action = next_action\n",
    "        return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CartPole with the Q-Learning agent <a name='qlagent' />\n",
    "\n",
    "### Preparation: Initialize the Q-learning agent and training parameters\n",
    "\n",
    "Beginning from an empty Q-table, QLearningAgent tries to learn $Q(s_t, a_t)$ by Q-learning with $\\epsilon$-greedy strategy in 50 episodes. The result gif shows the agent gradually learns from trials and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "q_agent = QLearningAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "max_length = 200\n",
    "initial_reward = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Train the Q-learning agent\n",
    "\n",
    "Each time the learner cell is executed, the agent improves its policy.  To start over from scratch with the agent, go back and execute the cell that instantiates the agent with `q_agent = QLearningAgent()` \n",
    "\n",
    "The result statistics of average and maximum steps achieved during the episodes shows that over time the agent is able to improve its behavior.  We can see this in the visualization as the CartPole is able to stay verticle for longer periods of time after more training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the agent - execute this cell as many times as you wish\n",
    "# set the visualize_plt flag to True to see the cart in the notebook.  \n",
    "# note that this will run slower if visualized\n",
    "steps = learner(agent=q_agent, episodes=num_episodes, max_length = max_length, \n",
    "                init_reward=initial_reward, visualize_plt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing the agent - run this smaller sampling after the agent is achieving success and NOT exploring\n",
    "\n",
    "num_episodes = 5\n",
    "max_length = 200\n",
    "initial_reward = 1\n",
    "steps = learner(agent=q_agent, episodes=num_episodes, max_length = max_length,\n",
    "            init_reward=initial_reward, mode='test') # set mode to 'test' to avoid exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Example: MountainCar-v0 <a name='mtn' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym is full of environments to try with varying difficulties and challenges.  Start by reviewing the information found in the [wiki](https://github.com/openai/gym/wiki/Table-of-environments).  Here's some information provided about the [MountainCar-v0 environment](https://github.com/openai/gym/wiki/MountainCar-v0):\n",
    "\n",
    "<img src=\"image/mtncar.png?\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "Get an under powered car to the top of a hill (top = 0.5 position).\n",
    "\n",
    "#### Observation\n",
    "Type: Box(2)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Position | -1.2 | 0.6\n",
    "1 | Velocity | -0.07 | 0.07\n",
    "\n",
    "#### Actions\n",
    "Type: Discrete(3)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | push left\n",
    "1 | no push\n",
    "2 | push right\n",
    "\n",
    "#### Reward\n",
    "-1 for each time step, until the goal position of 0.5 is reached. As with MountainCarContinuous v0, there is no penalty for climbing the left hill, which upon reached acts as a wall.\n",
    "\n",
    "#### Starting state\n",
    "Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    "#### Episode termination\n",
    "The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: MountainCar with the Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d30d6a414a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# try mountain car with the Random agent (default)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MountainCar-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e06d43dba839>\u001b[0m in \u001b[0;36mlearner\u001b[0;34m(agent, env_id, episodes, max_length, init_reward, ignore_done, visualize_plt, mode)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvisualize_plt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only call this once, only for jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# initialize the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXlibCanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# XXX ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(self, canvas)\u001b[0m\n\u001b[1;32m    321\u001b[0m         self.glx_window = glx.glXCreateWindow(\n\u001b[1;32m    322\u001b[0m             self.x_display, self.config._fbconfig, canvas.x_window, None)\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mset_current\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m         glx.glXMakeContextCurrent(\n\u001b[1;32m    327\u001b[0m             self.x_display, self.glx_window, self.glx_window, self.glx_context)\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibContext13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/base.py\u001b[0m in \u001b[0;36mset_current\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mgl_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_active_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mglu_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_active_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/gl_info.py\u001b[0m in \u001b[0;36mset_active_context\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m                                        c_char_p).value)\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglGetString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGL_VERSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglext_arb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglGetStringi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGL_NUM_EXTENSIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mnum_extensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGLint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/gl_info.py\u001b[0m in \u001b[0;36mhave_version\u001b[0;34m(self, major, minor, release)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s.0.0'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mimajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miminor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirelease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimajor\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mimajor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miminor\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyglet/gl/gl_info.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s.0.0'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mimajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miminor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirelease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimajor\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mimajor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miminor\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# try mountain car with the Random agent (default)\n",
    "steps = learner(env_id='MountainCar-v0', episodes=3, max_length = 200, init_reward=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: MountainCar with the Q-Learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the Q-Learning agent, we need to consider how the environment setup will affect the learning for our agent.  The MountainCar-v0 problem is a hard one because the agent can't really learn what actions are helpful until it actually climbs the hill successfully.  As long as that goal has not been met, all the episodes will have an accumulated reward of -1 for each timestep, or -200, since the number of steps allowed is capped.  Therefore, it's important that the car explores as many avenues as possible to solve the problem.  As a help for this notebook, we can extend the number of timesteps for the learner by ignoring the \"done\" signal and setting a higher number for the `max_length` parameter in learner. A special case provision has been included in the learner for this operation specifically for MountainCar-v0\n",
    "\n",
    "The Q-Learning agent also needs a way to discretize the observations for MountainCar, just as it did for the CartPole problem.  Although the agent itself will use the same basic iterative algorithm, we need to change the setup a bit.  This can be accomplished by setting up a different table of \"splits\".\n",
    "\n",
    "Whereas in CartPole, the split was:\n",
    "``` python\n",
    "\n",
    "    splits = [\n",
    "    # Position\n",
    "    np.linspace(-2.4, 2.4, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-3.5, 3.5, n_bins)[1:-1],\n",
    "    # Angle.\n",
    "    np.linspace(-0.5, 0.5, n_bins)[1:-1],\n",
    "    # Tip velocity\n",
    "    np.linspace(-2.0, 2.0, n_bins)[1:-1]\n",
    "    ]\n",
    " ```\n",
    " \n",
    "MountainCar only has two observation values:\n",
    "``` python\n",
    "    splits = [\n",
    "    # Position\n",
    "    np.linspace(-1.2, 0.6, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-0.07, 0.07, n_bins)[1:-1],\n",
    "    ]\n",
    "```\n",
    "In addition, the MountainCar problem has three actions whereas the CarPole only had two, so the agent will need to knw that . The Q-Learning agent defined previously has a provision for passing all the parameters it uses including the number of actions and splits.  Go ahead and create a split table now for the MountainCar Q-Learning agent and define the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO define the number of bins `n_bins` and a list name `splits_mtncar` for use by the Q-Learning agent.\n",
    "# The n_bins used for CartPole was 9, but feel free to experiment with this number\n",
    "n_bins = \n",
    "splits_mtncar = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO instantiate a QLearningAgent named q_agent_mtncar\n",
    "# you may want to tweak the hyper-parameters, such as the exploration rate, to increase\n",
    "#    the agent's chances of climbing the hill\n",
    "q_agent_mtncar = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the agent long enough for it to achieve success within 500 steps\n",
    "# Feel free to modify any of these parameters\n",
    "num_episodes = 50\n",
    "max_length = 2000\n",
    "steps = learner(agent=q_agent_mtncar, env_id='MountainCar-v0',\n",
    "                        episodes=num_episodes, max_length = max_length, init_reward=0,\n",
    "                        ignore_done=True, visualize_plt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent_mtncar.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent_mtncar.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing the agent - run this smaller sampling after the agent is achieving success\n",
    "num_episodes = 3\n",
    "max_length = 500\n",
    "steps = learner(agent=q_agent_mtncar, env_id='MountainCar-v0',\n",
    "                        episodes=num_episodes, max_length = max_length, init_reward=0,\n",
    "                        ignore_done=True, visualize_plt=True, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent_mtncar.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent_mtncar.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!  \n",
    "Now you are on your way to trying other environment problems and RL algorithms.  Feel free to add additional cells here in order to try additional environments in OpenAI Gym."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robo-env",
   "language": "python",
   "name": "robo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
